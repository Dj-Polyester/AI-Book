@misc{nowell_math_2006,
	title = {Math {Proofs}: {The} {Dot} {Product} and {Cosine}},
	url = {https://mathproofs.blogspot.com/2006/07/dot-product-and-cosine.html},
	urldate = {2024-05-30},
	author = {Nowell, Phil},
	month = jul,
	year = {2006},
	file = {Math Proofs\: The Dot Product and Cosine:/home/polyester/Zotero/storage/J4GM7HTY/dot-product-and-cosine.html:text/html},
}

@misc{magidin_linear_2011,
	title = {linear algebra - {Eigenvectors} of real symmetric matrices are orthogonal - {Mathematics} {Stack} {Exchange}},
	url = {https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal/82471},
	urldate = {2024-07-21},
	author = {Magidin, Arturo},
	month = nov,
	year = {2011},
	file = {linear algebra - Eigenvectors of real symmetric matrices are orthogonal - Mathematics Stack Exchange:/home/polyester/Zotero/storage/KNAE34N5/82471.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:/home/polyester/Zotero/storage/5X7E6JEC/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@misc{gundersen_reparameterization_2018,
	title = {The {Reparameterization} {Trick}},
	url = {https://gregorygundersen.com/blog/2018/04/29/reparameterization/},
	urldate = {2024-06-05},
	author = {Gundersen, Gregory},
	month = apr,
	year = {2018},
	file = {The Reparameterization Trick:/home/polyester/Zotero/storage/RK6CK8XU/reparameterization.html:text/html},
}

@misc{gupta_kl_2020,
	title = {{KL} {Divergence} between 2 {Gaussian} {Distributions}},
	url = {https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/},
	urldate = {2024-06-03},
	author = {Gupta, Rishabh},
	month = apr,
	year = {2020},
	file = {KL Divergence between 2 Gaussian Distributions:/home/polyester/Zotero/storage/D3A5C4RI/2020-04-16-kl-divergence-between-2-gaussian-distributions.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:/home/polyester/Zotero/storage/7GUQ2YRX/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{weng_what_2021,
	title = {What are {Diffusion} {Models}? {\textbar} {Lil}'{Log}},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	urldate = {2024-05-29},
	author = {Weng, Lilian},
	month = jul,
	year = {2021},
	file = {What are Diffusion Models? | Lil'Log:/home/polyester/Zotero/storage/JPK3Z3RN/2021-07-11-diffusion-models.html:text/html},
}
