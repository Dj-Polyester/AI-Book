@misc{nowell_math_2006,
	title = {Math {Proofs}: {The} {Dot} {Product} and {Cosine}},
	url = {https://mathproofs.blogspot.com/2006/07/dot-product-and-cosine.html},
	urldate = {2024-05-30},
	author = {Nowell, Phil},
	month = jul,
	year = {2006},
	file = {Math Proofs\: The Dot Product and Cosine:/home/polyester/Zotero/storage/J4GM7HTY/dot-product-and-cosine.html:text/html},
}

@misc{magidin_linear_2011,
	title = {linear algebra - {Eigenvectors} of real symmetric matrices are orthogonal - {Mathematics} {Stack} {Exchange}},
	url = {https://math.stackexchange.com/a/82471/731492},
	urldate = {2024-07-21},
	author = {Magidin, Arturo},
	month = nov,
	year = {2011},
	file = {linear algebra - Eigenvectors of real symmetric matrices are orthogonal - Mathematics Stack Exchange:/home/polyester/Zotero/storage/KNAE34N5/82471.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:/home/polyester/Zotero/storage/5X7E6JEC/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@misc{gundersen_reparameterization_2018,
	title = {The {Reparameterization} {Trick}},
	url = {https://gregorygundersen.com/blog/2018/04/29/reparameterization/},
	urldate = {2024-06-05},
	author = {Gundersen, Gregory},
	month = apr,
	year = {2018},
	file = {The Reparameterization Trick:/home/polyester/Zotero/storage/RK6CK8XU/reparameterization.html:text/html},
}

@misc{gupta_kl_2020,
	title = {{KL} {Divergence} between 2 {Gaussian} {Distributions}},
	url = {https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/},
	urldate = {2024-06-03},
	author = {Gupta, Rishabh},
	month = apr,
	year = {2020},
	file = {KL Divergence between 2 Gaussian Distributions:/home/polyester/Zotero/storage/D3A5C4RI/2020-04-16-kl-divergence-between-2-gaussian-distributions.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:/home/polyester/Zotero/storage/7GUQ2YRX/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{weng_diffusion_2021,
	title = {What are {Diffusion} {Models}? {\textbar} {Lil}'{Log}},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	urldate = {2024-05-29},
	author = {Weng, Lilian},
	month = jul,
	year = {2021},
}

@misc{cornell_2017,
	title = {Lecture 12: {Bias} {Variance} {Tradeoff}},
	url = {https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html},
	urldate = {2024-09-15},
        author = {Weinberger, Kilian},
        year = {2017},
	file = {Lecture 12\: Bias Variance Tradeoff:C\:\\Users\\mc316\\Zotero\\storage\\V2Y5RWHI\\lecturenote12.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.},
	language = {en},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {PDF:C\:\\Users\\mc316\\Zotero\\storage\\7N92L4NL\\Kingma ve Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf},
}

@misc{weng_vae_2018,
	title = {From {Autoencoder} to {Beta}-{VAE} {\textbar} {Lil}'{Log}},
	url = {https://lilianweng.github.io/posts/2018-08-12-vae/},
	urldate = {2024-09-21},
	author = {Weng, Lilian},
	month = aug,
	year = {2018},
}

@article{hochreiter_long_1997,
	title = {Long {Short} {Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf},
	number = {8},
	urldate = {2025-03-11},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
	year = {1997},
	pages = {1735--1780},
	file = {PDF:/home/polyester/Zotero/storage/ZG8HZAT6/LSTM.pdf:application/pdf},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {PDF:/home/polyester/Zotero/storage/4A5N4UM8/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf},
}

@misc{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	doi = {10.48550/arXiv.1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = mar,
	year = {2018},
	note = {arXiv:1802.05365 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	file = {PDF:/home/polyester/Zotero/storage/MP7FSYW6/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf},
}

@misc{parikh_decomposable_2016,
	title = {A {Decomposable} {Attention} {Model} for {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/1606.01933},
	doi = {10.48550/arXiv.1606.01933},
	abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	month = sep,
	year = {2016},
	note = {arXiv:1606.01933 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages, 1 figure, Proceeedings of EMNLP 2016},
	file = {PDF:/home/polyester/Zotero/storage/T98M78Q5/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Language Inference.pdf:application/pdf},
}

@misc{seo_bidirectional_2018,
	title = {Bidirectional {Attention} {Flow} for {Machine} {Comprehension}},
	url = {http://arxiv.org/abs/1611.01603},
	doi = {10.48550/arXiv.1611.01603},
	abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a ﬁxed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention ﬂow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
	month = jun,
	year = {2018},
	note = {arXiv:1611.01603 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {PDF:/home/polyester/Zotero/storage/CAWAY45W/Seo et al. - 2018 - Bidirectional Attention Flow for Machine Comprehension.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {PDF:/home/polyester/Zotero/storage/FKRH9DVZ/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/L39IHNS4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {PDF:/home/polyester/Zotero/storage/JI46WE7D/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	language = {en},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/2JR7DA26/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	file = {PDF:/home/polyester/Zotero/storage/D5PBQ3AN/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {PDF:/home/polyester/Zotero/storage/VM7KDILE/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{schick_exploiting_2021,
	title = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/2001.07676},
	doi = {10.48550/arXiv.2001.07676},
	abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Schick, Timo and Schütze, Hinrich},
	month = jan,
	year = {2021},
	note = {arXiv:2001.07676 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at EACL2021},
	file = {PDF:/home/polyester/Zotero/storage/MEX8KWK7/Schick and Schütze - 2021 - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.pdf:application/pdf},
}

@book{lapan_deep_2020,
	address = {Place of publication not identified},
	edition = {2nd ed},
	title = {Deep reinforcement learning hands-on: apply modern {RL} methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, {Second} edition},
	isbn = {978-1-83882-699-4 978-1-83882-004-6},
	shorttitle = {Deep reinforcement learning hands-on},
	abstract = {New edition of the bestselling guide to deep reinforcement learning and how it's used to solve complex real-world problems. Revised and expanded to include multi-agent methods, discrete optimization, RL in robotics, advanced exploration techniques, and more Key Features Second edition of the bestselling introduction to deep reinforcement learning, expanded with six new chapters Learn advanced exploration techniques including noisy networks, pseudo-count, and network distillation methods Apply RL methods to cheap hardware robotics platforms Book Description Deep Reinforcement Learning Hands-On, Second Edition is an updated and expanded version of the bestselling guide to the very latest reinforcement learning (RL) tools and techniques. It provides you with an introduction to the fundamentals of RL, along with the hands-on ability to code intelligent learning agents to perform a range of practical tasks. With six new chapters devoted to a variety of up-to-the-minute developments in RL, including discrete optimization (solving the Rubik's Cube), multi-agent methods, Microsoft's TextWorld environment, advanced exploration techniques, and more, you will come away from this book with a deep understanding of the latest innovations in this emerging field. In addition, you will gain actionable insights into such topic areas as deep Q-networks, policy gradient methods, continuous control problems, and highly scalable, non-gradient methods. You will also discover how to build a real hardware robot trained with RL for less than},
	language = {en},
	publisher = {Packt Publishing},
	editor = {Lapan, Maxim},
	year = {2020},
	file = {PDF:/home/polyester/Zotero/storage/79SLGZLZ/Lapan - 2020 - Deep reinforcement learning hands-on apply modern RL methods to practical problems of chatbots, rob.pdf:application/pdf},
}